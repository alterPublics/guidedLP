# Pytest configuration for Guided Label Propagation library
# Optimized for scientific computing and network analysis testing

[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Output and reporting
addopts = 
    # Verbose output with detailed test information
    -v
    # Show local variables in tracebacks for debugging
    --tb=short
    # Enable strict mode for better test quality
    --strict-config
    --strict-markers
    # Show summary of failed tests
    -rf
    # Show extra test summary info
    -ra
    # Enable coverage reporting
    --cov=src
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-report=xml
    # Coverage configuration for scientific code
    --cov-config=.coveragerc
    # Performance and timeout settings
    --timeout=300
    --timeout-method=thread
    # Parallel execution for faster testing
    -n auto
    # Memory monitoring for large dataset tests
    --memray
    # Benchmark integration
    --benchmark-only-during-commit

# Test markers for organizing different types of tests
markers =
    # Core functionality markers
    unit: Unit tests for individual functions and classes
    integration: Integration tests that test multiple components together
    end_to_end: End-to-end tests that test complete workflows
    
    # Domain-specific markers
    network: Tests related to network construction and analysis
    glp: Tests for Guided Label Propagation algorithms
    timeseries: Tests for temporal network analysis
    validation: Tests for model validation and performance metrics
    
    # Performance and scale markers
    slow: Tests that take more than 10 seconds to run
    fast: Tests that take less than 1 second to run
    memory_intensive: Tests that require significant memory (>1GB)
    cpu_intensive: Tests that require significant CPU resources
    
    # Data size markers
    small_data: Tests with small datasets (<1000 nodes)
    medium_data: Tests with medium datasets (1000-10000 nodes)
    large_data: Tests with large datasets (>10000 nodes)
    
    # Algorithm-specific markers
    numerical: Tests involving numerical computations and precision
    algorithmic: Tests for algorithm correctness and convergence
    stochastic: Tests involving random processes (may need multiple runs)
    
    # Infrastructure markers
    io: Tests involving file I/O operations
    visualization: Tests that generate plots or visualizations
    external: Tests that depend on external resources or network access
    
    # Quality markers
    regression: Regression tests that check for specific bug fixes
    smoke: Basic smoke tests for core functionality
    acceptance: Acceptance tests for user-facing features

# Test filtering and collection  
collect_ignore = setup.py docs/ build/ dist/ .eggs/ examples/

# Minimum version requirements
minversion = 7.0

# Log configuration for debugging scientific computations
log_level = INFO
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Warning filters for scientific libraries
filterwarnings =
    # Ignore common warnings from scientific libraries
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    # NetworkIt C++ warnings
    ignore::RuntimeWarning:networkit.*
    # NumPy precision warnings in tests
    ignore::RuntimeWarning:numpy.*
    # Polars optimization warnings
    ignore::UserWarning:polars.*
    # Matplotlib backend warnings
    ignore::UserWarning:matplotlib.*
    # SciPy sparse matrix warnings
    ignore::SparseEfficiencyWarning:scipy.sparse.*
    # Scikit-learn deprecation warnings
    ignore::FutureWarning:sklearn.*
    # Turn specific warnings into errors for our code
    error::UserWarning:src.*
    error::RuntimeWarning:src.*

# Test execution configuration
# Timeout for individual tests (5 minutes default, longer for large data tests)
timeout = 300

# JUnit XML output for CI/CD integration
junit_family = xunit2
junit_logging = all
junit_log_passing_tests = true

# Temporary directory configuration
tmp_path_retention_count = 3
tmp_path_retention_policy = failed

# Parameterization configuration
# Generate test IDs that are readable and deterministic
parametrize_names_type = "tuple"

# Test discovery patterns for scientific computing
# Include hypothesis testing strategies
hypothesis_profiles =
    default: hypothesis.strategies
    fast: hypothesis.strategies with max_examples=10
    thorough: hypothesis.strategies with max_examples=1000

# Performance testing configuration
# Benchmark configuration for performance regression testing
benchmark_group_by = "group"
benchmark_sort = "mean"
benchmark_compare_fail = "mean:10%"  # Fail if performance degrades by >10%
benchmark_warmup = true
benchmark_warmup_iterations = 3
benchmark_min_rounds = 5

# Memory usage monitoring
# Configure memory tracking for large dataset tests
memory_profiler_backend = "tracemalloc"

# Doctest configuration
# Enable doctests in source code
doctest_modules = true
doctest_optionflags = 
    NORMALIZE_WHITESPACE
    IGNORE_EXCEPTION_DETAIL
    ELLIPSIS

# Plugin configuration
# Enable useful plugins for scientific computing tests
plugins = 
    pytest-cov
    pytest-xdist
    pytest-timeout
    pytest-benchmark
    pytest-memray
    pytest-mock
    pytest-hypothesis
    pytest-doctests

# Test data configuration
# Configure test fixtures and sample data
cache_dir = .pytest_cache
fixtures_per_test = 10

# Network testing specific configuration
# Configure network testing parameters
network_test_timeout = 60
max_test_graph_size = 10000  # Maximum nodes in test graphs
test_data_seed = 42  # Seed for reproducible test data generation

# Numerical precision configuration for scientific tests
float_tolerance = 1e-10  # Default tolerance for floating point comparisons
relative_tolerance = 1e-6  # Relative tolerance for scientific computations

# Parallel execution configuration
# Configure pytest-xdist for parallel test execution
dist = loadgroup  # Distribute tests by test file groupings
tx = popen  # Use subprocess for test execution

# Test selection shortcuts
# Define test suites for different purposes
quick = "not slow and not memory_intensive and not cpu_intensive"
standard = "not memory_intensive and not external"
comprehensive = ""  # Run all tests
nightly = "slow or memory_intensive or large_data"  # Long-running tests for nightly builds